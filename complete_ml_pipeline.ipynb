{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Project Overview: House Price Prediction"
      ],
      "metadata": {
        "id": "PxkoHr-xamMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this tutorial, we'll build a house price prediction pipeline using the California Housing dataset. This project demonstrates all essential ML pipeline components while solving a practical real-world problem."
      ],
      "metadata": {
        "id": "YhA9Z5f6ao5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Business Problem\n",
        "\n",
        "\n",
        "Predict house prices based on location, house characteristics, and demographic data to help real estate professionals make informed decisions."
      ],
      "metadata": {
        "id": "hJk3NGcqayW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Step 1: Environment Setup and Data Ingestion"
      ],
      "metadata": {
        "id": "4ms6AjsNa9Ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pip install pandas numpy matplotlib seaborn scikit-learn joblib jupyter"
      ],
      "metadata": {
        "id": "97AP3oPBbBEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3j2_RC9aDnU"
      },
      "outputs": [],
      "source": [
        "# Required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ML Pipeline Environment Ready!\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Data Ingestion Pipeline Class"
      ],
      "metadata": {
        "id": "5O3ijNM8bIfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataIngestion:\n",
        "    \"\"\"Handles data loading and initial validation\"\"\"\n",
        "\n",
        "    def __init__(self, data_path=None):\n",
        "        self.raw_data = None\n",
        "        self.feature_names = None\n",
        "        self.target_name = None\n",
        "        self.data_path = data_path\n",
        "\n",
        "    def load_california_housing_data(self):\n",
        "        \"\"\"Load California housing dataset\"\"\"\n",
        "        try:\n",
        "            # Load dataset\n",
        "            california_housing = fetch_california_housing()\n",
        "\n",
        "            # Create DataFrame\n",
        "            self.raw_data = pd.DataFrame(\n",
        "                california_housing.data,\n",
        "                columns=california_housing.feature_names\n",
        "            )\n",
        "            self.raw_data['target'] = california_housing.target\n",
        "            self.feature_names = list(california_housing.feature_names)\n",
        "            self.target_name = 'target'\n",
        "\n",
        "            print(\"Data loaded successfully!\")\n",
        "            print(f\"Dataset shape: {self.raw_data.shape}\")\n",
        "            return self.raw_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def load_custom_data(self, file_path):\n",
        "        \"\"\"Load custom dataset from CSV file\"\"\"\n",
        "        try:\n",
        "            if file_path.endswith('.csv'):\n",
        "                self.raw_data = pd.read_csv(file_path)\n",
        "            elif file_path.endswith('.xlsx'):\n",
        "                self.raw_data = pd.read_excel(file_path)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format. Use CSV or Excel.\")\n",
        "\n",
        "            print(f\"Custom data loaded successfully!\")\n",
        "            print(f\"Dataset shape: {self.raw_data.shape}\")\n",
        "            return self.raw_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading custom data: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def basic_data_info(self):\n",
        "        \"\"\"Display basic information about the dataset\"\"\"\n",
        "        if self.raw_data is not None:\n",
        "            print(\"\\nDataset Information:\")\n",
        "            print(f\"Shape: {self.raw_data.shape}\")\n",
        "            print(f\"Memory usage: {self.raw_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "            print(\"\\nColumn Information:\")\n",
        "            print(self.raw_data.info())\n",
        "\n",
        "            print(\"\\nFirst 5 rows:\")\n",
        "            print(self.raw_data.head())\n",
        "\n",
        "            print(\"\\nBasic Statistics:\")\n",
        "            print(self.raw_data.describe())\n",
        "\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_data_quality(self):\n",
        "        \"\"\"Check for data quality issues\"\"\"\n",
        "        if self.raw_data is None:\n",
        "            return False\n",
        "\n",
        "        print(\"\\nData Quality Check:\")\n",
        "\n",
        "        # Check for missing values\n",
        "        missing_values = self.raw_data.isnull().sum()\n",
        "        if missing_values.sum() > 0:\n",
        "            print(\"Missing Values Found:\")\n",
        "            print(missing_values[missing_values > 0])\n",
        "        else:\n",
        "            print(\"No missing values found\")\n",
        "\n",
        "        # Check for duplicates\n",
        "        duplicates = self.raw_data.duplicated().sum()\n",
        "        print(f\"Duplicate rows: {duplicates}\")\n",
        "\n",
        "        # Check for infinite values\n",
        "        numeric_cols = self.raw_data.select_dtypes(include=[np.number]).columns\n",
        "        inf_values = np.isinf(self.raw_data[numeric_cols]).sum().sum()\n",
        "        print(f\"Infinite values: {inf_values}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "# Initialize and load data\n",
        "data_ingestion = DataIngestion()\n",
        "df = data_ingestion.load_california_housing_data()\n",
        "data_ingestion.basic_data_info()\n",
        "data_ingestion.check_data_quality()"
      ],
      "metadata": {
        "id": "6g7BW2g7bJeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 2: Data Exploration and Analysis"
      ],
      "metadata": {
        "id": "mGLrIdOxbNI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exploratory Data Analysis Class"
      ],
      "metadata": {
        "id": "Zg5-Q9PxbT9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataExploration:\n",
        "    \"\"\"Comprehensive data exploration and visualization\"\"\"\n",
        "\n",
        "    def __init__(self, data, target_column, feature_columns):\n",
        "        self.data = data.copy()\n",
        "        self.target_column = target_column\n",
        "        self.feature_columns = feature_columns\n",
        "\n",
        "    def statistical_summary(self):\n",
        "        \"\"\"Generate comprehensive statistical summary\"\"\"\n",
        "        print(\"Statistical Summary:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Basic statistics\n",
        "        print(\"\\nDescriptive Statistics:\")\n",
        "        print(self.data[self.feature_columns + [self.target_column]].describe())\n",
        "\n",
        "        # Correlation with target\n",
        "        correlations = self.data[self.feature_columns].corrwith(self.data[self.target_column])\n",
        "        print(f\"\\nCorrelations with {self.target_column}:\")\n",
        "        print(correlations.sort_values(ascending=False))\n",
        "\n",
        "    def visualize_distributions(self):\n",
        "        \"\"\"Create distribution plots for all features\"\"\"\n",
        "        n_features = len(self.feature_columns)\n",
        "        n_cols = 3\n",
        "        n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "        plt.figure(figsize=(15, 5 * n_rows))\n",
        "\n",
        "        for i, column in enumerate(self.feature_columns):\n",
        "            plt.subplot(n_rows, n_cols, i + 1)\n",
        "            plt.hist(self.data[column], bins=30, alpha=0.7, edgecolor='black')\n",
        "            plt.title(f'Distribution of {column}')\n",
        "            plt.xlabel(column)\n",
        "            plt.ylabel('Frequency')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def correlation_heatmap(self):\n",
        "        \"\"\"Create correlation heatmap\"\"\"\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        correlation_matrix = self.data[self.feature_columns + [self.target_column]].corr()\n",
        "\n",
        "        sns.heatmap(correlation_matrix,\n",
        "                   annot=True,\n",
        "                   cmap='coolwarm',\n",
        "                   center=0,\n",
        "                   square=True,\n",
        "                   fmt='.2f')\n",
        "        plt.title('Feature Correlation Heatmap')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def target_analysis(self):\n",
        "        \"\"\"Analyze target variable\"\"\"\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Target distribution\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.hist(self.data[self.target_column], bins=50, alpha=0.7, edgecolor='black')\n",
        "        plt.title(f'Distribution of {self.target_column}')\n",
        "        plt.xlabel(self.target_column)\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        # Box plot\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.boxplot(self.data[self.target_column])\n",
        "        plt.title(f'Box Plot of {self.target_column}')\n",
        "        plt.ylabel(self.target_column)\n",
        "\n",
        "        # Q-Q plot\n",
        "        plt.subplot(1, 3, 3)\n",
        "        from scipy import stats\n",
        "        stats.probplot(self.data[self.target_column], dist=\"norm\", plot=plt)\n",
        "        plt.title(f'Q-Q Plot of {self.target_column}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def feature_target_relationships(self):\n",
        "        \"\"\"Visualize relationships between features and target\"\"\"\n",
        "        n_features = len(self.feature_columns)\n",
        "        n_cols = 3\n",
        "        n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "        plt.figure(figsize=(15, 5 * n_rows))\n",
        "\n",
        "        for i, column in enumerate(self.feature_columns):\n",
        "            plt.subplot(n_rows, n_cols, i + 1)\n",
        "            plt.scatter(self.data[column], self.data[self.target_column], alpha=0.5)\n",
        "            plt.xlabel(column)\n",
        "            plt.ylabel(self.target_column)\n",
        "            plt.title(f'{column} vs {self.target_column}')\n",
        "\n",
        "            # Add trend line\n",
        "            z = np.polyfit(self.data[column], self.data[self.target_column], 1)\n",
        "            p = np.poly1d(z)\n",
        "            plt.plot(self.data[column], p(self.data[column]), \"r--\", alpha=0.8)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Perform data exploration\n",
        "explorer = DataExploration(df, 'target', data_ingestion.feature_names)\n",
        "explorer.statistical_summary()\n",
        "explorer.visualize_distributions()\n",
        "explorer.correlation_heatmap()\n",
        "explorer.target_analysis()\n",
        "explorer.feature_target_relationships()"
      ],
      "metadata": {
        "id": "teXOzCCWbRMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 3: Data Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "D1h66bejbe78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Comprehensive Data Preprocessing Class"
      ],
      "metadata": {
        "id": "MYhKIr2jbmVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessor:\n",
        "    \"\"\"Handle all data preprocessing steps\"\"\"\n",
        "\n",
        "    def __init__(self, target_column):\n",
        "        self.target_column = target_column\n",
        "        self.scaler = None\n",
        "        self.feature_columns = None\n",
        "        self.preprocessing_steps = []\n",
        "\n",
        "    def handle_missing_values(self, data, strategy='mean'):\n",
        "        \"\"\"Handle missing values in the dataset\"\"\"\n",
        "        print(\"Handling missing values...\")\n",
        "\n",
        "        if data.isnull().sum().sum() == 0:\n",
        "            print(\"No missing values found\")\n",
        "            return data\n",
        "\n",
        "        numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "        categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "        data_processed = data.copy()\n",
        "\n",
        "        # Handle numeric columns\n",
        "        for col in numeric_columns:\n",
        "            if data_processed[col].isnull().sum() > 0:\n",
        "                if strategy == 'mean':\n",
        "                    fill_value = data_processed[col].mean()\n",
        "                elif strategy == 'median':\n",
        "                    fill_value = data_processed[col].median()\n",
        "                elif strategy == 'mode':\n",
        "                    fill_value = data_processed[col].mode()[0]\n",
        "                else:\n",
        "                    fill_value = 0\n",
        "\n",
        "                data_processed[col].fillna(fill_value, inplace=True)\n",
        "                print(f\"Filled {col} missing values with {strategy}: {fill_value:.2f}\")\n",
        "\n",
        "        # Handle categorical columns\n",
        "        for col in categorical_columns:\n",
        "            if data_processed[col].isnull().sum() > 0:\n",
        "                fill_value = data_processed[col].mode()[0]\n",
        "                data_processed[col].fillna(fill_value, inplace=True)\n",
        "                print(f\"Filled {col} missing values with mode: {fill_value}\")\n",
        "\n",
        "        self.preprocessing_steps.append(f\"Missing values handled using {strategy} strategy\")\n",
        "        return data_processed\n",
        "\n",
        "    def remove_outliers(self, data, method='iqr', threshold=1.5):\n",
        "        \"\"\"Remove outliers from the dataset\"\"\"\n",
        "        print(f\"Removing outliers using {method} method...\")\n",
        "\n",
        "        data_processed = data.copy()\n",
        "        numeric_columns = data_processed.select_dtypes(include=[np.number]).columns\n",
        "        numeric_columns = [col for col in numeric_columns if col != self.target_column]\n",
        "\n",
        "        initial_shape = data_processed.shape[0]\n",
        "\n",
        "        if method == 'iqr':\n",
        "            for column in numeric_columns:\n",
        "                Q1 = data_processed[column].quantile(0.25)\n",
        "                Q3 = data_processed[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - threshold * IQR\n",
        "                upper_bound = Q3 + threshold * IQR\n",
        "\n",
        "                outliers_mask = (data_processed[column] >= lower_bound) & (data_processed[column] <= upper_bound)\n",
        "                data_processed = data_processed[outliers_mask]\n",
        "\n",
        "        elif method == 'zscore':\n",
        "            from scipy import stats\n",
        "            for column in numeric_columns:\n",
        "                z_scores = np.abs(stats.zscore(data_processed[column]))\n",
        "                data_processed = data_processed[z_scores < threshold]\n",
        "\n",
        "        final_shape = data_processed.shape[0]\n",
        "        removed_count = initial_shape - final_shape\n",
        "\n",
        "        print(f\"Removed {removed_count} outliers ({removed_count/initial_shape*100:.2f}% of data)\")\n",
        "        self.preprocessing_steps.append(f\"Outliers removed using {method} method: {removed_count} rows\")\n",
        "\n",
        "        return data_processed\n",
        "\n",
        "    def scale_features(self, X_train, X_test, method='standard'):\n",
        "        \"\"\"Scale features using specified method\"\"\"\n",
        "        print(f\"Scaling features using {method} scaling...\")\n",
        "\n",
        "        if method == 'standard':\n",
        "            self.scaler = StandardScaler()\n",
        "        elif method == 'robust':\n",
        "            self.scaler = RobustScaler()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported scaling method\")\n",
        "\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        # Convert back to DataFrame\n",
        "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "        self.preprocessing_steps.append(f\"Features scaled using {method} scaling\")\n",
        "\n",
        "        return X_train_scaled, X_test_scaled\n",
        "\n",
        "    def create_train_test_split(self, data, test_size=0.2, random_state=42):\n",
        "        \"\"\"Create train-test split\"\"\"\n",
        "        print(f\"Creating train-test split (test_size={test_size})...\")\n",
        "\n",
        "        # Separate features and target\n",
        "        X = data.drop(columns=[self.target_column])\n",
        "        y = data[self.target_column]\n",
        "\n",
        "        self.feature_columns = X.columns.tolist()\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y,\n",
        "            test_size=test_size,\n",
        "            random_state=random_state,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        print(f\"Training set shape: {X_train.shape}\")\n",
        "        print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "        self.preprocessing_steps.append(f\"Data split into train ({1-test_size:.0%}) and test ({test_size:.0%})\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def get_preprocessing_summary(self):\n",
        "        \"\"\"Get summary of all preprocessing steps\"\"\"\n",
        "        print(\"\\nPreprocessing Summary:\")\n",
        "        print(\"=\" * 50)\n",
        "        for i, step in enumerate(self.preprocessing_steps, 1):\n",
        "            print(f\"{i}. {step}\")\n",
        "\n",
        "# Apply preprocessing\n",
        "preprocessor = DataPreprocessor('target')\n",
        "\n",
        "# Handle missing values and outliers\n",
        "df_processed = preprocessor.handle_missing_values(df, strategy='median')\n",
        "df_processed = preprocessor.remove_outliers(df_processed, method='iqr', threshold=1.5)\n",
        "\n",
        "# Create train-test split\n",
        "X_train, X_test, y_train, y_test = preprocessor.create_train_test_split(df_processed)\n",
        "\n",
        "# Scale features\n",
        "X_train_scaled, X_test_scaled = preprocessor.scale_features(X_train, X_test, method='standard')\n",
        "\n",
        "# Get preprocessing summary\n",
        "preprocessor.get_preprocessing_summary()"
      ],
      "metadata": {
        "id": "uqs8h5hKbk-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 4: Feature Engineering"
      ],
      "metadata": {
        "id": "4YKb8XrGbus8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Advanced Feature Engineering Class"
      ],
      "metadata": {
        "id": "KKJaeg89bxie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEngineer:\n",
        "    \"\"\"Create and transform features for better model performance\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.new_features = []\n",
        "        self.feature_importance = {}\n",
        "\n",
        "    def create_polynomial_features(self, X, degree=2, interaction_only=False):\n",
        "        \"\"\"Create polynomial features\"\"\"\n",
        "        from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "        print(f\"Creating polynomial features (degree={degree})...\")\n",
        "\n",
        "        poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, include_bias=False)\n",
        "        X_poly = poly.fit_transform(X)\n",
        "\n",
        "        # Get feature names\n",
        "        feature_names = poly.get_feature_names_out(X.columns)\n",
        "        X_poly_df = pd.DataFrame(X_poly, columns=feature_names, index=X.index)\n",
        "\n",
        "        self.new_features.append(f\"Polynomial features (degree={degree})\")\n",
        "        print(f\"Created {X_poly_df.shape[1] - X.shape[1]} new polynomial features\")\n",
        "\n",
        "        return X_poly_df\n",
        "\n",
        "    def create_domain_specific_features(self, X):\n",
        "        \"\"\"Create domain-specific features for housing data\"\"\"\n",
        "        print(\"Creating domain-specific features...\")\n",
        "\n",
        "        X_engineered = X.copy()\n",
        "\n",
        "        # Rooms per person\n",
        "        if 'AveRooms' in X.columns and 'AveOccup' in X.columns:\n",
        "            X_engineered['RoomsPerPerson'] = X_engineered['AveRooms'] / X_engineered['AveOccup']\n",
        "            self.new_features.append('RoomsPerPerson')\n",
        "\n",
        "        # Bedrooms ratio\n",
        "        if 'AveBedrms' in X.columns and 'AveRooms' in X.columns:\n",
        "            X_engineered['BedroomsRatio'] = X_engineered['AveBedrms'] / X_engineered['AveRooms']\n",
        "            self.new_features.append('BedroomsRatio')\n",
        "\n",
        "        # Population density\n",
        "        if 'Population' in X.columns and 'AveOccup' in X.columns:\n",
        "            X_engineered['PopulationDensity'] = X_engineered['Population'] / X_engineered['AveOccup']\n",
        "            self.new_features.append('PopulationDensity')\n",
        "\n",
        "        # Income per room\n",
        "        if 'MedInc' in X.columns and 'AveRooms' in X.columns:\n",
        "            X_engineered['IncomePerRoom'] = X_engineered['MedInc'] / X_engineered['AveRooms']\n",
        "            self.new_features.append('IncomePerRoom')\n",
        "\n",
        "        # Age categories\n",
        "        if 'HouseAge' in X.columns:\n",
        "            X_engineered['AgeCategory'] = pd.cut(X_engineered['HouseAge'],\n",
        "                                               bins=[0, 10, 25, 40, float('inf')],\n",
        "                                               labels=['New', 'Recent', 'Mature', 'Old'])\n",
        "            X_engineered['AgeCategory'] = X_engineered['AgeCategory'].astype(str)\n",
        "            self.new_features.append('AgeCategory')\n",
        "\n",
        "        # Location clustering (simplified)\n",
        "        if 'Latitude' in X.columns and 'Longitude' in X.columns:\n",
        "            # Create location clusters based on lat/long\n",
        "            from sklearn.cluster import KMeans\n",
        "            location_features = X_engineered[['Latitude', 'Longitude']]\n",
        "            kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "            X_engineered['LocationCluster'] = kmeans.fit_predict(location_features)\n",
        "            self.new_features.append('LocationCluster')\n",
        "\n",
        "        print(f\"Created {len(self.new_features)} domain-specific features\")\n",
        "        return X_engineered\n",
        "\n",
        "    def select_features(self, X, y, method='correlation', k=10):\n",
        "        \"\"\"Select best features using specified method\"\"\"\n",
        "        from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "\n",
        "        print(f\"Selecting top {k} features using {method} method...\")\n",
        "\n",
        "        # Handle categorical features\n",
        "        X_numeric = X.select_dtypes(include=[np.number])\n",
        "\n",
        "        if method == 'correlation':\n",
        "            # Use correlation with target\n",
        "            correlations = X_numeric.corrwith(y).abs()\n",
        "            top_features = correlations.nlargest(k).index.tolist()\n",
        "\n",
        "        elif method == 'f_regression':\n",
        "            selector = SelectKBest(score_func=f_regression, k=k)\n",
        "            selector.fit(X_numeric, y)\n",
        "            top_features = X_numeric.columns[selector.get_support()].tolist()\n",
        "\n",
        "        elif method == 'mutual_info':\n",
        "            selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
        "            selector.fit(X_numeric, y)\n",
        "            top_features = X_numeric.columns[selector.get_support()].tolist()\n",
        "\n",
        "        print(f\"Selected features: {top_features}\")\n",
        "        return top_features\n",
        "\n",
        "    def get_feature_engineering_summary(self):\n",
        "        \"\"\"Get summary of feature engineering steps\"\"\"\n",
        "        print(\"\\nFeature Engineering Summary:\")\n",
        "        print(\"=\" * 50)\n",
        "        for i, feature in enumerate(self.new_features, 1):\n",
        "            print(f\"{i}. {feature}\")\n",
        "\n",
        "# Apply feature engineering\n",
        "feature_engineer = FeatureEngineer()\n",
        "\n",
        "# Create domain-specific features\n",
        "X_train_engineered = feature_engineer.create_domain_specific_features(X_train_scaled)\n",
        "X_test_engineered = feature_engineer.create_domain_specific_features(X_test_scaled)\n",
        "\n",
        "# Handle categorical features for modeling\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "categorical_columns = X_train_engineered.select_dtypes(include=['object']).columns\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    X_train_engineered[col] = le.fit_transform(X_train_engineered[col].astype(str))\n",
        "    X_test_engineered[col] = le.transform(X_test_engineered[col].astype(str))\n",
        "\n",
        "# Select best features\n",
        "numeric_features = X_train_engineered.select_dtypes(include=[np.number])\n",
        "top_features = feature_engineer.select_features(numeric_features, y_train, method='correlation', k=15)\n",
        "\n",
        "# Keep only selected features\n",
        "X_train_final = X_train_engineered[top_features]\n",
        "X_test_final = X_test_engineered[top_features]\n",
        "\n",
        "print(f\"\\nFinal feature set shape: {X_train_final.shape}\")\n",
        "feature_engineer.get_feature_engineering_summary()"
      ],
      "metadata": {
        "id": "Vf_Du0hkbieF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 5: Model Training and Validation"
      ],
      "metadata": {
        "id": "PXlAkew5b3bW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Comprehensive Model Training Pipeline"
      ],
      "metadata": {
        "id": "Ghp1NkcVcj-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    \"\"\"Train and validate multiple machine learning models\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.trained_models = {}\n",
        "        self.model_scores = {}\n",
        "        self.best_model = None\n",
        "        self.best_model_name = None\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize different models for comparison\"\"\"\n",
        "        self.models = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'Ridge Regression': Ridge(random_state=42),\n",
        "            'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "            'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
        "        }\n",
        "\n",
        "        print(f\"Initialized {len(self.models)} models for training\")\n",
        "\n",
        "    def train_models(self, X_train, y_train):\n",
        "        \"\"\"Train all models\"\"\"\n",
        "        print(\"Training models...\")\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"Training {name}...\")\n",
        "            model.fit(X_train, y_train)\n",
        "            self.trained_models[name] = model\n",
        "\n",
        "        print(\"All models trained successfully!\")\n",
        "\n",
        "    def cross_validate_models(self, X_train, y_train, cv=5):\n",
        "        \"\"\"Perform cross-validation for all models\"\"\"\n",
        "        print(f\"Performing {cv}-fold cross-validation...\")\n",
        "\n",
        "        cv_results = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            # Perform cross-validation\n",
        "            cv_scores = cross_val_score(model, X_train, y_train,\n",
        "                                      cv=cv, scoring='neg_mean_squared_error')\n",
        "\n",
        "            cv_results[name] = {\n",
        "                'CV_MSE_Mean': -cv_scores.mean(),\n",
        "                'CV_MSE_Std': cv_scores.std(),\n",
        "                'CV_RMSE_Mean': np.sqrt(-cv_scores.mean()),\n",
        "                'CV_Scores': cv_scores\n",
        "            }\n",
        "\n",
        "            print(f\"{name}: CV RMSE = {np.sqrt(-cv_scores.mean()):.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
        "\n",
        "        self.cv_results = cv_results\n",
        "        return cv_results\n",
        "\n",
        "    def hyperparameter_tuning(self, X_train, y_train, model_name='Random Forest'):\n",
        "        \"\"\"Perform hyperparameter tuning for specified model\"\"\"\n",
        "        print(f\"Performing hyperparameter tuning for {model_name}...\")\n",
        "\n",
        "        if model_name == 'Random Forest':\n",
        "            param_grid = {\n",
        "                'n_estimators': [100, 200, 300],\n",
        "                'max_depth': [10, 20, None],\n",
        "                'min_samples_split': [2, 5, 10],\n",
        "                'min_samples_leaf': [1, 2, 4]\n",
        "            }\n",
        "            model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "        elif model_name == 'Gradient Boosting':\n",
        "            param_grid = {\n",
        "                'n_estimators': [100, 200, 300],\n",
        "                'learning_rate': [0.05, 0.1, 0.15],\n",
        "                'max_depth': [3, 5, 7],\n",
        "                'subsample': [0.8, 0.9, 1.0]\n",
        "            }\n",
        "            model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "        else:\n",
        "            print(f\"Hyperparameter tuning not implemented for {model_name}\")\n",
        "            return None\n",
        "\n",
        "        # Perform grid search\n",
        "        grid_search = GridSearchCV(\n",
        "            model, param_grid,\n",
        "            cv=3, scoring='neg_mean_squared_error',\n",
        "            n_jobs=-1, verbose=1\n",
        "        )\n",
        "\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        print(f\"Best parameters for {model_name}:\")\n",
        "        print(grid_search.best_params_)\n",
        "        print(f\"Best CV score: {np.sqrt(-grid_search.best_score_):.4f}\")\n",
        "\n",
        "        # Update the model with best parameters\n",
        "        self.trained_models[f\"{model_name}_Tuned\"] = grid_search.best_estimator_\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "A4EOWXA-b3J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Setp 6: Model Evaluation and Selection"
      ],
      "metadata": {
        "id": "J_KOKfFpcymP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Advanced Model Evaluation"
      ],
      "metadata": {
        "id": "VwAD200Qc4jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluator:\n",
        "    \"\"\"Comprehensive model evaluation and analysis\"\"\"\n",
        "\n",
        "    def __init__(self, model, model_name):\n",
        "        self.model = model\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def detailed_evaluation(self, X_test, y_test):\n",
        "        \"\"\"Perform detailed model evaluation\"\"\"\n",
        "        print(f\"Detailed Evaluation for {self.model_name}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = self.model.predict(X_test)\n",
        "\n",
        "        # Calculate all metrics\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "        print(f\"RÂ² Score: {r2:.4f}\")\n",
        "        print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "\n",
        "        # Residual analysis\n",
        "        residuals = y_test - y_pred\n",
        "        print(f\"\\nResidual Analysis:\")\n",
        "        print(f\"Mean Residual: {np.mean(residuals):.4f}\")\n",
        "        print(f\"Std Residual: {np.std(residuals):.4f}\")\n",
        "\n",
        "        return {\n",
        "            'predictions': y_pred,\n",
        "            'residuals': residuals,\n",
        "            'metrics': {\n",
        "                'MSE': mse, 'RMSE': rmse, 'MAE': mae,\n",
        "                'R2': r2, 'MAPE': mape\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def plot_predictions(self, y_test, y_pred):\n",
        "        \"\"\"Plot actual vs predicted values\"\"\"\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Actual vs Predicted scatter plot\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual Values')\n",
        "        plt.ylabel('Predicted Values')\n",
        "        plt.title(f'{self.model_name}: Actual vs Predicted')\n",
        "\n",
        "        # Residual plot\n",
        "        plt.subplot(1, 3, 2)\n",
        "        residuals = y_test - y_pred\n",
        "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predicted Values')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title(f'{self.model_name}: Residual Plot')\n",
        "\n",
        "        # Residual distribution\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
        "        plt.xlabel('Residuals')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(f'{self.model_name}: Residual Distribution')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def learning_curves(self, X_train, y_train, cv=5):\n",
        "        \"\"\"Plot learning curves to check for overfitting\"\"\"\n",
        "        from sklearn.model_selection import learning_curve\n",
        "\n",
        "        print(\"Generating learning curves...\")\n",
        "\n",
        "        train_sizes, train_scores, val_scores = learning_curve(\n",
        "            self.model, X_train, y_train, cv=cv,\n",
        "            train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "            scoring='neg_mean_squared_error', n_jobs=-1\n",
        "        )\n",
        "\n",
        "        # Calculate mean and std\n",
        "        train_rmse_mean = np.sqrt(-train_scores.mean(axis=1))\n",
        "        train_rmse_std = np.sqrt(train_scores.std(axis=1))\n",
        "        val_rmse_mean = np.sqrt(-val_scores.mean(axis=1))\n",
        "        val_rmse_std = np.sqrt(val_scores.std(axis=1))\n",
        "\n",
        "        # Plot learning curves\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(train_sizes, train_rmse_mean, 'o-', label='Training RMSE')\n",
        "        plt.fill_between(train_sizes, train_rmse_mean - train_rmse_std,\n",
        "                         train_rmse_mean + train_rmse_std, alpha=0.1)\n",
        "\n",
        "        plt.plot(train_sizes, val_rmse_mean, 'o-', label='Validation RMSE')\n",
        "        plt.fill_between(train_sizes, val_rmse_mean - val_rmse_std,\n",
        "                         val_rmse_mean + val_rmse_std, alpha=0.1)\n",
        "\n",
        "        plt.xlabel('Training Set Size')\n",
        "        plt.ylabel('RMSE')\n",
        "        plt.title(f'{self.model_name}: Learning Curves')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# Evaluate the best model\n",
        "evaluator = ModelEvaluator(trainer.best_model, trainer.best_model_name)\n",
        "evaluation_results = evaluator.detailed_evaluation(X_test_final, y_test)\n",
        "evaluator.plot_predictions(y_test, evaluation_results['predictions'])\n",
        "evaluator.learning_curves(X_train_final, y_train)"
      ],
      "metadata": {
        "id": "gRBMnLHMcnIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 7: Model Deployment Pipeline"
      ],
      "metadata": {
        "id": "ezwRviBZc7xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Simple Deployment and Prediction System"
      ],
      "metadata": {
        "id": "VCHoNF19dB6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelDeployment:\n",
        "    \"\"\"Handle model deployment and prediction serving\"\"\"\n",
        "\n",
        "    def __init__(self, model, preprocessor, feature_engineer, feature_columns):\n",
        "        self.model = model\n",
        "        self.preprocessor = preprocessor\n",
        "        self.feature_engineer = feature_engineer\n",
        "        self.feature_columns = feature_columns\n",
        "        self.deployment_info = {}\n",
        "\n",
        "    def save_model_artifacts(self, model_path='model_artifacts'):\n",
        "        \"\"\"Save all model artifacts for deployment\"\"\"\n",
        "        import os\n",
        "        from datetime import datetime\n",
        "\n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "        # Save model\n",
        "        model_file = os.path.join(model_path, 'trained_model.pkl')\n",
        "        joblib.dump(self.model, model_file)\n",
        "\n",
        "        # Save preprocessor\n",
        "        preprocessor_file = os.path.join(model_path, 'preprocessor.pkl')\n",
        "        joblib.dump(self.preprocessor, preprocessor_file)\n",
        "\n",
        "        # Save feature columns\n",
        "        feature_file = os.path.join(model_path, 'feature_columns.pkl')\n",
        "        joblib.dump(self.feature_columns, feature_file)\n",
        "\n",
        "        # Save deployment info\n",
        "        self.deployment_info = {\n",
        "            'model_type': type(self.model).__name__,\n",
        "            'feature_count': len(self.feature_columns),\n",
        "            'deployment_date': datetime.now().isoformat(),\n",
        "            'model_path': model_file,\n",
        "            'preprocessor_path': preprocessor_file,\n",
        "            'feature_path': feature_file\n",
        "        }\n",
        "\n",
        "        info_file = os.path.join(model_path, 'deployment_info.pkl')\n",
        "        joblib.dump(self.deployment_info, info_file)\n",
        "\n",
        "        print(f\"Model artifacts saved to: {model_path}\")\n",
        "        print(f\"Model type: {self.deployment_info['model_type']}\")\n",
        "        print(f\"Feature count: {self.deployment_info['feature_count']}\")\n",
        "\n",
        "    def load_model_artifacts(self, model_path='model_artifacts'):\n",
        "        \"\"\"Load model artifacts for inference\"\"\"\n",
        "        import os\n",
        "\n",
        "        try:\n",
        "            # Load model\n",
        "            model_file = os.path.join(model_path, 'trained_model.pkl')\n",
        "            self.model = joblib.load(model_file)\n",
        "\n",
        "            # Load preprocessor\n",
        "            preprocessor_file = os.path.join(model_path, 'preprocessor.pkl')\n",
        "            self.preprocessor = joblib.load(preprocessor_file)\n",
        "\n",
        "            # Load feature columns\n",
        "            feature_file = os.path.join(model_path, 'feature_columns.pkl')\n",
        "            self.feature_columns = joblib.load(feature_file)\n",
        "\n",
        "            # Load deployment info\n",
        "            info_file = os.path.join(model_path, 'deployment_info.pkl')\n",
        "            self.deployment_info = joblib.load(info_file)\n",
        "\n",
        "            print(\"Model artifacts loaded successfully!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model artifacts: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def predict_single(self, input_data):\n",
        "        \"\"\"Make prediction for a single input\"\"\"\n",
        "        try:\n",
        "            # Convert to DataFrame if it's a dictionary\n",
        "            if isinstance(input_data, dict):\n",
        "                input_df = pd.DataFrame([input_data])\n",
        "            else:\n",
        "                input_df = input_data.copy()\n",
        "\n",
        "            # Apply the same preprocessing steps\n",
        "            # Note: This is simplified - in practice, you'd need to apply\n",
        "            # the exact same preprocessing pipeline\n",
        "\n",
        "            # Ensure all required features are present\n",
        "            missing_features = set(self.feature_columns) - set(input_df.columns)\n",
        "            if missing_features:\n",
        "                raise ValueError(f\"Missing features: {missing_features}\")\n",
        "\n",
        "            # Select only the features used in training\n",
        "            input_processed = input_df[self.feature_columns]\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = self.model.predict(input_processed)\n",
        "\n",
        "            return {\n",
        "                'prediction': float(prediction[0]),\n",
        "                'input_features': input_data,\n",
        "                'model_type': type(self.model).__name__\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'prediction': None\n",
        "            }\n",
        "\n",
        "    def predict_batch(self, input_data):\n",
        "        \"\"\"Make predictions for multiple inputs\"\"\"\n",
        "        try:\n",
        "            # Ensure input is DataFrame\n",
        "            if isinstance(input_data, list):\n",
        "                input_df = pd.DataFrame(input_data)\n",
        "            else:\n",
        "                input_df = input_data.copy()\n",
        "\n",
        "            # Select only the features used in training\n",
        "            input_processed = input_df[self.feature_columns]\n",
        "\n",
        "            # Make predictions\n",
        "            predictions = self.model.predict(input_processed)\n",
        "\n",
        "            return {\n",
        "                'predictions': predictions.tolist(),\n",
        "                'count': len(predictions),\n",
        "                'model_type': type(self.model).__name__\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'predictions': None\n",
        "            }\n",
        "\n",
        "    def create_prediction_api(self):\n",
        "        \"\"\"Create a simple prediction API structure\"\"\"\n",
        "        api_code = '''\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load model artifacts at startup\n",
        "model = joblib.load('model_artifacts/trained_model.pkl')\n",
        "preprocessor = joblib.load('model_artifacts/preprocessor.pkl')\n",
        "feature_columns = joblib.load('model_artifacts/feature_columns.pkl')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        # Get input data\n",
        "        data = request.json\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        input_df = pd.DataFrame([data])\n",
        "\n",
        "        # Preprocess and predict\n",
        "        input_processed = input_df[feature_columns]\n",
        "        prediction = model.predict(input_processed)\n",
        "\n",
        "        return jsonify({\n",
        "            'prediction': float(prediction[0]),\n",
        "            'status': 'success'\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            'error': str(e),\n",
        "            'status': 'error'\n",
        "        })\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({'status': 'healthy'})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
        "'''\n",
        "\n",
        "        with open('prediction_api.py', 'w') as f:\n",
        "            f.write(api_code)\n",
        "\n",
        "        print(\"Prediction API created: prediction_api.py\")\n",
        "        print(\"Run with: python prediction_api.py\")\n",
        "\n",
        "    def create_requirements_file(self):\n",
        "        \"\"\"Create requirements.txt for deployment\"\"\"\n",
        "        requirements = [\n",
        "            'pandas>=1.3.0',\n",
        "            'numpy>=1.21.0',\n",
        "            'scikit-learn>=1.0.0',\n",
        "            'joblib>=1.0.0',\n",
        "            'matplotlib>=3.4.0',\n",
        "            'seaborn>=0.11.0',\n",
        "            'flask>=2.0.0'\n",
        "        ]\n",
        "\n",
        "        with open('requirements.txt', 'w') as f:\n",
        "            for req in requirements:\n",
        "                f.write(req + '\\n')\n",
        "\n",
        "        print(\"Requirements file created: requirements.txt\")\n",
        "\n",
        "# Deploy the model\n",
        "deployment = ModelDeployment(\n",
        "    model=trainer.best_model,\n",
        "    preprocessor=preprocessor,\n",
        "    feature_engineer=feature_engineer,\n",
        "    feature_columns=X_train_final.columns.tolist()\n",
        ")\n",
        "\n",
        "# Save model artifacts\n",
        "deployment.save_model_artifacts()\n",
        "\n",
        "# Create API and requirements\n",
        "deployment.create_prediction_api()\n",
        "deployment.create_requirements_file()\n",
        "\n",
        "# Test prediction\n",
        "sample_input = {\n",
        "    'MedInc': 8.3252,\n",
        "    'HouseAge': 41.0,\n",
        "    'AveRooms': 6.984,\n",
        "    'AveBedrms': 1.024,\n",
        "    'Population': 322.0,\n",
        "    'AveOccup': 2.556,\n",
        "    'Latitude': 37.88,\n",
        "    'Longitude': -122.23\n",
        "}\n",
        "\n",
        "# Note: This is a simplified example. The actual input would need to match\n",
        "# the exact features used in training after feature engineering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "sDvzakh5dGSU",
        "outputId": "e83631bd-decf-4488-ff96-8abb9abd8488"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-4169829970.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;31m# Deploy the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m deployment = ModelDeployment(\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mfeature_engineer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_engineer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 8: Testing and Validation"
      ],
      "metadata": {
        "id": "OlDGC0kMdElQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Comprehensive Testing Suite"
      ],
      "metadata": {
        "id": "znOBoOXodc6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTester:\n",
        "    \"\"\"Comprehensive testing for ML pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, deployment):\n",
        "        self.deployment = deployment\n",
        "        self.test_results = {}\n",
        "\n",
        "    def test_data_quality(self, test_data):\n",
        "        \"\"\"Test data quality and integrity\"\"\"\n",
        "        print(\"Testing Data Quality...\")\n",
        "\n",
        "        tests = {\n",
        "            'no_missing_values': test_data.isnull().sum().sum() == 0,\n",
        "            'no_infinite_values': np.isfinite(test_data.select_dtypes(include=[np.number])).all().all(),\n",
        "            'positive_target': (test_data['target'] > 0).all() if 'target' in test_data.columns else True,\n",
        "            'reasonable_ranges': True  # Add specific range checks\n",
        "        }\n",
        "\n",
        "        print(\"Data Quality Tests:\")\n",
        "        for test_name, result in tests.items():\n",
        "            status = \"PASS\" if result else \"FAIL\"\n",
        "            print(f\"  {test_name}: {status}\")\n",
        "\n",
        "        return tests\n",
        "\n",
        "    def test_model_performance(self, X_test, y_test, thresholds):\n",
        "        \"\"\"Test model performance against thresholds\"\"\"\n",
        "        print(\"Testing Model Performance...\")\n",
        "\n",
        "        y_pred = self.deployment.model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        tests = {\n",
        "            'rmse_threshold': rmse < thresholds.get('rmse', float('inf')),\n",
        "            'mae_threshold': mae < thresholds.get('mae', float('inf')),\n",
        "            'r2_threshold': r2 > thresholds.get('r2', 0)\n",
        "        }\n",
        "\n",
        "        print(\"Performance Tests:\")\n",
        "        for test_name, result in tests.items():\n",
        "            status = \"PASS\" if result else \"FAIL\"\n",
        "            print(f\"  {test_name}: {status}\")\n",
        "\n",
        "        print(f\"  Actual RMSE: {rmse:.4f}\")\n",
        "        print(f\"  Actual MAE: {mae:.4f}\")\n",
        "        print(f\"  Actual RÂ²: {r2:.4f}\")\n",
        "\n",
        "        return tests\n",
        "\n",
        "    def test_prediction_consistency(self, test_inputs, n_runs=5):\n",
        "        \"\"\"Test prediction consistency across multiple runs\"\"\"\n",
        "        print(\"Testing Prediction Consistency...\")\n",
        "\n",
        "        predictions = []\n",
        "        for i in range(n_runs):\n",
        "            pred = self.deployment.predict_batch(test_inputs)\n",
        "            if pred['predictions'] is not None:\n",
        "                predictions.append(pred['predictions'])\n",
        "\n",
        "        if predictions:\n",
        "            # Calculate coefficient of variation\n",
        "            predictions_array = np.array(predictions)\n",
        "            cv = np.std(predictions_array, axis=0) / np.mean(predictions_array, axis=0)\n",
        "\n",
        "            # Test if predictions are consistent (CV < 0.01)\n",
        "            consistency_test = (cv < 0.01).all()\n",
        "\n",
        "            print(f\"  Consistency test: {'PASS' if consistency_test else 'FAIL'}\")\n",
        "            print(f\"  Average CV: {np.mean(cv):.6f}\")\n",
        "\n",
        "            return consistency_test\n",
        "\n",
        "        return False\n",
        "\n",
        "    def test_edge_cases(self):\n",
        "        \"\"\"Test model behavior with edge cases\"\"\"\n",
        "        print(\"Testing Edge Cases...\")\n",
        "\n",
        "        # Test with extreme values\n",
        "        edge_cases = [\n",
        "            {'MedInc': 0.1, 'HouseAge': 1.0, 'AveRooms': 1.0, 'AveBedrms': 0.1,\n",
        "             'Population': 1.0, 'AveOccup': 1.0, 'Latitude': 32.0, 'Longitude': -124.0},\n",
        "            {'MedInc': 15.0, 'HouseAge': 52.0, 'AveRooms': 20.0, 'AveBedrms': 5.0,\n",
        "             'Population': 10000.0, 'AveOccup': 10.0, 'Latitude': 42.0, 'Longitude': -114.0}\n",
        "        ]\n",
        "\n",
        "        tests = {\n",
        "            'handles_extreme_low': True,\n",
        "            'handles_extreme_high': True,\n",
        "            'no_negative_predictions': True\n",
        "        }\n",
        "\n",
        "        for i, case in enumerate(edge_cases):\n",
        "            try:\n",
        "                result = self.deployment.predict_single(case)\n",
        "                if result['prediction'] is None:\n",
        "                    tests[f'handles_extreme_{\"low\" if i == 0 else \"high\"}'] = False\n",
        "                elif result['prediction'] < 0:\n",
        "                    tests['no_negative_predictions'] = False\n",
        "            except Exception:\n",
        "                tests[f'handles_extreme_{\"low\" if i == 0 else \"high\"}'] = False\n",
        "\n",
        "        print(\"Edge Case Tests:\")\n",
        "        for test_name, result in tests.items():\n",
        "            status = \"PASS\" if result else \"FAIL\"\n",
        "            print(f\"  {test_name}: {status}\")\n",
        "\n",
        "        return tests\n",
        "\n",
        "    def generate_test_report(self, all_tests):\n",
        "        \"\"\"Generate comprehensive test report\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ML PIPELINE TEST REPORT\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        total_tests = sum(len(test_group) for test_group in all_tests.values())\n",
        "        passed_tests = sum(sum(test_group.values()) for test_group in all_tests.values())\n",
        "\n",
        "        print(f\"Total Tests: {total_tests}\")\n",
        "        print(f\"Passed Tests: {passed_tests}\")\n",
        "        print(f\"Failed Tests: {total_tests - passed_tests}\")\n",
        "        print(f\"Pass Rate: {passed_tests/total_tests*100:.1f}%\")\n",
        "\n",
        "        print(\"\\nDetailed Results:\")\n",
        "        for category, tests in all_tests.items():\n",
        "            print(f\"\\n{category}:\")\n",
        "            for test_name, result in tests.items():\n",
        "                status = \"â PASS\" if result else \"â FAIL\"\n",
        "                print(f\"  {test_name}: {status}\")\n",
        "\n",
        "        return {\n",
        "            'total_tests': total_tests,\n",
        "            'passed_tests': passed_tests,\n",
        "            'pass_rate': passed_tests/total_tests*100\n",
        "        }\n",
        "\n",
        "# Run comprehensive tests\n",
        "tester = ModelTester(deployment)\n",
        "\n",
        "# Define performance thresholds\n",
        "performance_thresholds = {\n",
        "    'rmse': 1.0,  # Adjust based on your requirements\n",
        "    'mae': 0.8,\n",
        "    'r2': 0.6\n",
        "}\n",
        "\n",
        "# Run all tests\n",
        "all_test_results = {}\n",
        "all_test_results['Data Quality'] = tester.test_data_quality(df_processed)\n",
        "all_test_results['Model Performance'] = tester.test_model_performance(\n",
        "    X_test_final, y_test, performance_thresholds\n",
        ")\n",
        "all_test_results['Prediction Consistency'] = {'consistency': tester.test_prediction_consistency(X_test_final.head())}\n",
        "all_test_results['Edge Cases'] = tester.test_edge_cases()\n",
        "\n",
        "# Generate final report\n",
        "test_report = tester.generate_test_report(all_test_results)"
      ],
      "metadata": {
        "id": "1kbFmoRqc7Vf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}